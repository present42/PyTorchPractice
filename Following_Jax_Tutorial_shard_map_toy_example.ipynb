{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/present42/PyTorchPractice/blob/main/Following_Jax_Tutorial_shard_map_toy_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WYfrbQBBpixd"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from jax.sharding import Mesh, NamedSharding, PartitionSpec as P\n",
        "from jax.experimental.shard_map import shard_map"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matrix Multiplication"
      ],
      "metadata": {
        "id": "ZYNaq3JDqFPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mesh = Mesh(jax.devices()[:4], ('i', ))\n",
        "\n",
        "def device_put(x, pspec):\n",
        "  return jax.device_put(x, NamedSharding(mesh, pspec))"
      ],
      "metadata": {
        "id": "l869GNy4p06w"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1: `all-gather` on one side"
      ],
      "metadata": {
        "id": "lQRNAl2lqJFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider a matrix multiplication where we shard the left-hand side argument on its leading dimension:"
      ],
      "metadata": {
        "id": "CLdXFQ9SqWzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lhs_spec = P('i', None)\n",
        "lhs = device_put(jax.random.normal(jax.random.key(0), (8, 8)), lhs_spec)"
      ],
      "metadata": {
        "id": "xyFPRS6sp_XQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rhs_spec = P('i', None)\n",
        "rhs = device_put(jax.random.normal(jax.random.key(1), (8, 4)), rhs_spec)"
      ],
      "metadata": {
        "id": "Wu6BdmV0qotP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jax.debug.visualize_array_sharding(lhs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "IRJjDNK2qu63",
        "outputId": "3e9a272f-d035-49df-9a56-1e7847a80581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m          \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m          \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;222;158;214m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;222;158;214m          \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mTPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m          \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;222;158;214m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m                         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m          \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74mTPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m          \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m                         \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;181;207;107m                         \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;181;207;107m          \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107mTPU 3\u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m          \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;181;207;107m                         \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">          TPU 0          </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">          TPU 1          </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">                         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">          TPU 2          </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">                         </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">                         </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">          TPU 3          </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">                         </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform this matrix multiplication, we can first all-gather the right-hand side and then perform local matrix multiplication against the sharded left-hand side:"
      ],
      "metadata": {
        "id": "JbP7ffr6raqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial"
      ],
      "metadata": {
        "id": "2-2yr0d2r0O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Review\n",
        " - `in_specs` identifies some of the corresponding input array's axes with mesh axes by name using `PartitionSpec`s, representing how to split that input into the blocks to which the body function is applied.\n",
        " - `out_specs` identifies some of the corresponding output array's axes with mesh axes by name, represnting how the output blocks should be assembled back together to form the final output value."
      ],
      "metadata": {
        "id": "bekmYLJcspbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "@partial(shard_map, mesh=mesh, in_specs=(lhs_spec, rhs_spec), out_specs=rhs_spec)\n",
        "def matmul_allgather(lhs_block, rhs_block):\n",
        "  # all-gather : gathering array shards along an axis,\n",
        "  #              so that each function application has a full copy of the data\n",
        "  #              along that axis\n",
        "  rhs = jax.lax.all_gather(rhs_block, 'i', tiled=True)\n",
        "  return lhs_block @ rhs"
      ],
      "metadata": {
        "id": "2ZKxjvWZrO-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = matmul_allgather(lhs, rhs)\n",
        "print(jnp.allclose(out, lhs @ rhs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPIJkNCtscsW",
        "outputId": "29f7a3f7-8594-4a1e-a17e-a3c9eec5099a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're not getting any compute/communication overlap here: before we can start the matmul, we need the `all_gather` to complete."
      ],
      "metadata": {
        "id": "X5nNcAZKtj75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import lax"
      ],
      "metadata": {
        "id": "Ay3LuQjCuwO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "@partial(shard_map, mesh=mesh, in_specs=(lhs_spec, rhs_spec), out_specs=rhs_spec)\n",
        "def matmul_allgather_overlapped(lhs_block, rhs_block):\n",
        "  size = jax.lax.psum(1, 'i')  # number of devices\n",
        "  idx = jax.lax.axis_index('i')\n",
        "\n",
        "  shift = partial(jax.lax.ppermute, axis_name='i', perm=[(i, (i + 1) % size) for i in range(size)])\n",
        "\n",
        "  B = lhs_block.shape[1] // size\n",
        "\n",
        "  # start_index: i * B / slice_size: B / axis: 1\n",
        "  lhs_blocks = lambda i: lax.dynamic_slice_in_dim(lhs_block, i * B, B, 1)\n",
        "\n",
        "  out_block = lhs_blocks(idx) @ rhs_block\n",
        "  for i in range(1, size):\n",
        "    rhs_block = shift(rhs_block)\n",
        "    out_block += lhs_blocks((idx - i) % size) @ rhs_block\n",
        "  return out_block"
      ],
      "metadata": {
        "id": "Z-0Jg-_Gsihu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = matmul_allgather_overlapped(lhs, rhs)\n",
        "print(jnp.allclose(out, lhs @ rhs, atol=1e-3, rtol=1e-3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZbAvT6Tvben",
        "outputId": "ec137e14-b209-4807-9e95-2a620e69c5b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The implementation allows overlap between communication and computation, and also avoids gathering a large intermediate onto each device. But on TPU it uses only half the interconnect bandwidth by permuting in only one direction along the ring."
      ],
      "metadata": {
        "id": "NvlGjZ8txGwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "@partial(shard_map, mesh=mesh, in_specs=(lhs_spec, rhs_spec), out_specs=rhs_spec)\n",
        "def matmul_allgather_overlapped_bidi(lhs_block, rhs_block):\n",
        "  size = jax.lax.psum(1, 'i')\n",
        "  idx = jax.lax.axis_index('i')\n",
        "  shift_up = partial(jax.lax.ppermute, axis_name='i', perm=[(i, (i + 1) % size) for i in range(size)])\n",
        "  shift_dn = partial(jax.lax.ppermute, axis_name='i', perm=[(i, (i - 1) % size) for i in range(size)])\n",
        "\n",
        "  B = lhs_block.shape[1] // size // 2\n",
        "  lhs_blocks = lambda i, hi: lax.dynamic_slice_in_dim(lhs_block, (2 * i + hi) * B, B, 1)\n",
        "\n",
        "  rhs_block_lo, rhs_block_hi = jnp.split(rhs_block, 2, axis=0)\n",
        "  out_block  = lhs_blocks(idx, 0) @ rhs_block_lo\n",
        "  out_block += lhs_blocks(idx, 1) @ rhs_block_hi\n",
        "  for i in range(1, size):\n",
        "    rhs_block_lo = shift_up(rhs_block_lo)\n",
        "    rhs_block_hi = shift_dn(rhs_block_hi)\n",
        "    out_block += lhs_blocks((idx - i) % size, 0) @ rhs_block_lo\n",
        "    out_block += lhs_blocks((idx + i) % size, 1) @ rhs_block_hi\n",
        "  return out_block"
      ],
      "metadata": {
        "id": "JOJM9IH3vmtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = matmul_allgather_overlapped_bidi(lhs, rhs)\n",
        "print(jnp.allclose(out, lhs @ rhs, atol=1e-3, rtol=1e-3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18MHEInQy8uz",
        "outputId": "843d24ac-8efe-47cd-c574-2055c4a7cf3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wN_RfmG3zCgK",
        "outputId": "8bd025af-68b2-4688-c2db-4bc3e07d0343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[ 0.4204327 , -3.2336767 ,  0.46415687,  2.3247633 ],\n",
              "       [-3.5942657 , -0.24846068, -1.9459741 ,  1.9542422 ],\n",
              "       [-2.1133657 ,  0.61801076, -0.18297681,  3.210961  ],\n",
              "       [-0.55151576, -3.1808214 ,  1.4808508 ,  0.21939349],\n",
              "       [-0.9624958 ,  5.5949306 ,  1.8143425 ,  0.3755707 ],\n",
              "       [-3.1725793 , -1.1635897 ,  1.3896794 , -0.84395945],\n",
              "       [ 1.0931401 , -2.4149542 ,  1.5044949 ,  1.3278372 ],\n",
              "       [ 0.1603303 , -0.08382738, -0.7018988 , -1.267864  ]],      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "@partial(shard_map, mesh=mesh, in_specs=(lhs_spec, rhs_spec),\n",
        "         out_specs=rhs_spec)\n",
        "def matmul_allgather_overlapped_bidi(lhs_block, rhs_block):\n",
        "  size = jax.lax.psum(1, 'i')\n",
        "  idx = jax.lax.axis_index('i')\n",
        "  shift_up = partial(jax.lax.ppermute, axis_name='i',\n",
        "                     perm=[(i, (i + 1) % size) for i in range(size)])\n",
        "  shift_dn = partial(jax.lax.ppermute, axis_name='i',\n",
        "                     perm=[(i, (i - 1) % size) for i in range(size)])\n",
        "\n",
        "  B = lhs_block.shape[1] // size // 2  # half-size blocks\n",
        "  lhs_blocks = lambda i, hi: lax.dynamic_slice_in_dim(lhs_block, (2*i+hi) * B, B, 1)\n",
        "\n",
        "  rhs_block_lo, rhs_block_hi = jnp.split(rhs_block, 2, axis=0)\n",
        "  out_block  = lhs_blocks(idx, 0) @ rhs_block_lo\n",
        "  out_block += lhs_blocks(idx, 1) @ rhs_block_hi\n",
        "  for i in range(1, size):\n",
        "    rhs_block_lo = shift_up(rhs_block_lo)\n",
        "    rhs_block_hi = shift_dn(rhs_block_hi)\n",
        "    out_block += lhs_blocks((idx - i) % size, 0) @ rhs_block_lo\n",
        "    out_block += lhs_blocks((idx + i) % size, 1) @ rhs_block_hi\n",
        "  return out_block"
      ],
      "metadata": {
        "id": "raRJmPl40DVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = matmul_allgather_overlapped_bidi(lhs, rhs)\n",
        "print(jnp.allclose(out, lhs @ rhs, atol=1e-3, rtol=1e-3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7soPoD9-0aVq",
        "outputId": "60dc3655-6260-408c-ac5d-ef1a7a6e176f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sjfw7He0btz",
        "outputId": "7bb56df3-7465-44e3-ca51-7d76201956bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[ 0.4204327 , -3.2336767 ,  0.46415687,  2.3247633 ],\n",
              "       [-3.5942657 , -0.24846068, -1.9459741 ,  1.9542422 ],\n",
              "       [-2.1133657 ,  0.61801076, -0.18297681,  3.210961  ],\n",
              "       [-0.55151576, -3.1808214 ,  1.4808508 ,  0.21939349],\n",
              "       [-0.9624958 ,  5.5949306 ,  1.8143425 ,  0.3755707 ],\n",
              "       [-3.1725793 , -1.1635897 ,  1.3896794 , -0.84395945],\n",
              "       [ 1.0931401 , -2.4149542 ,  1.5044949 ,  1.3278372 ],\n",
              "       [ 0.1603303 , -0.08382738, -0.7018988 , -1.267864  ]],      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lhs_spec = P(None, 'i')\n",
        "lhs = device_put(lhs, lhs_spec) # Divide the data column by column"
      ],
      "metadata": {
        "id": "-3-1onja0eaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jax.debug.visualize_array_sharding(lhs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "ftDud41GoNwx",
        "outputId": "62933fb9-c012-4d91-8bb5-6536d5ce07e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mTPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m  \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74mTPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m  \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m  \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107mTPU 3\u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m  \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  TPU 0  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">  TPU 1  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">  TPU 2  </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">  TPU 3  </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rhs_spec = P('i', None) # Divide the data row by row\n",
        "rhs = device_put(rhs, rhs_spec)"
      ],
      "metadata": {
        "id": "AoCF2nHyn-EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jax.debug.visualize_array_sharding(rhs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "i7uaXPOOoOjU",
        "outputId": "8fa484ff-270b-4861-86c1-f715fa81c487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m    \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;222;158;214m   \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mTPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m    \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m   \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74mTPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m    \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;173;73;74m            \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;181;207;107m   \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107mTPU 3\u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m    \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;181;207;107m            \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">   TPU 0    </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">   TPU 1    </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">   TPU 2    </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">            </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">   TPU 3    </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">            </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "@partial(shard_map, mesh=mesh, in_specs=(lhs_spec, rhs_spec), out_specs=rhs_spec)\n",
        "def matmul_psumscatter(lhs_block, rhs_block):\n",
        "  out_summand = lhs_block @ rhs_block\n",
        "  return jax.lax.psum_scatter(out_summand, 'i', tiled=True)\n",
        "\n",
        "out = matmul_psumscatter(lhs, rhs)\n",
        "print(jnp.allclose(out, lhs @ rhs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgmpcUz4oI9x",
        "outputId": "9882c5e6-6ac5-47ce-988e-416f7a51de76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But the scattering communication must wait for the entire local matrix multiply to finish before it can start."
      ],
      "metadata": {
        "id": "NsG3dwSPo8mN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@partial(shard_map, mesh=mesh, in_specs=(lhs_spec, rhs_spec), out_specs=rhs_spec)\n",
        "def matmul_psumscatter_overlapped(lhs_block, rhs_block):\n",
        "  size = jax.lax.psum(1, 'i')\n",
        "  idx = jax.lax.axis_index('i')\n",
        "  # interleave the communication steps with local matrix multiplication\n",
        "  shift = partial(jax.lax.ppermute, axis_name='i',\n",
        "                  perm=[(i, (i - 1) % size) for i in range(size)])\n",
        "  # shape of lhs_block (A, B // size)\n",
        "  # changed shape      (size, A // size, B // size)\n",
        "  # i.e. divide the rows as well\n",
        "  lhs_block = lhs_block.reshape(size, -1, lhs_block.shape[1])\n",
        "\n",
        "  # device idx computes partial result = idx-th row of col idx @ row idx\n",
        "  # This partial result will contribute to the idx-th row of the final result\n",
        "  # For device 0, here it computes local contribution to the second row of the final result\n",
        "  out_summand = lhs_block[(idx + 1) % size] @ rhs_block\n",
        "  for i in range(1, size):\n",
        "    # here, we send out_summand to the next gpu\n",
        "    # at the same time, we recieve out_summand from the previous gpu\n",
        "    # ex. From device 0 -> device (N - 1) -> ... -> device 1\n",
        "    # ex. From device 1 -> device 0 -> ... ->       device 2\n",
        "    out_summand = shift(out_summand)\n",
        "    #           out_summand: idx + i + 1 // current index: idx\n",
        "    # ex. i = 1 out_summand:\n",
        "    out_summand += lhs_block[(idx + i + 1) % size] @ rhs_block\n",
        "  return out_summand"
      ],
      "metadata": {
        "id": "rUiO7ijJow50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = matmul_psumscatter_overlapped(lhs, rhs)\n",
        "print(jnp.allclose(out, lhs @ rhs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r62FTPiktngf",
        "outputId": "545b1b35-765b-4ecc-f6eb-14ceb89e1422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@partial(shard_map, mesh=mesh, in_specs=(lhs_spec, rhs_spec), out_specs=rhs_spec)\n",
        "def matmul_psumscatter_overlapped_bidi(lhs_block, rhs_block):\n",
        "  size = jax.lax.psum(1, 'i')\n",
        "  idx = jax.lax.axis_index('i')\n",
        "  shift_up = partial(jax.lax.ppermute, axis_name='i',\n",
        "                     perm=[(i, (i + 1) % size) for i in range(size)])\n",
        "  shift_dn = partial(jax.lax.ppermute, axis_name='i',\n",
        "                     perm=[(i, (i - 1) % size) for i in range(size)])\n",
        "\n",
        "  B = lhs_block.shape[0] // size // 2 # half-sized blocks\n",
        "  lhs_blocks = lambda i, hi: jax.lax.dynamic_slice_in_dim(\n",
        "      lhs_block,\n",
        "      (2 * i + hi) * B,\n",
        "      B,\n",
        "      0\n",
        "  )\n",
        "\n",
        "  # out_summand_lo will be shifted to the next gpu\n",
        "  # it'll end up at (idx - 1)-th gpu\n",
        "  out_summand_lo = lhs_blocks((idx - 1) % size, 0) @ rhs_block\n",
        "  # out_summand_hi will be shifted to the prev gpu\n",
        "  # it'll end up at (idx + 1)-th gpu\n",
        "  out_summand_hi = lhs_blocks((idx + 1) % size, 1) @ rhs_block\n",
        "\n",
        "  for i in range(1, size):\n",
        "    out_summand_lo = shift_up(out_summand_lo)\n",
        "    out_summand_hi = shift_dn(out_summand_hi)\n",
        "\n",
        "    # Let's say i'm device 0\n",
        "    # I receive the copy from gpu N-1\n",
        "    # This output corresponds to (N-2)-th row\n",
        "    out_summand_lo += lhs_blocks((idx - i - 1) % size, 0) @ rhs_block\n",
        "\n",
        "    # Let's say i'm device 0\n",
        "    # received the copy from gpu 1\n",
        "    # This output corresponds to 3rd row (idx = 2)\n",
        "    out_summand_hi += lhs_blocks((idx + i + 1) % size, 1) @ rhs_block\n",
        "\n",
        "  return jnp.concatenate([out_summand_lo, out_summand_hi])"
      ],
      "metadata": {
        "id": "c9Xge8Q-trPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3IlnaT7_v8D2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out = matmul_psumscatter_overlapped_bidi(lhs, rhs)\n",
        "print(jnp.allclose(out, lhs @ rhs, atol=1e-2, rtol=1e-2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NeqsXnqv6lV",
        "outputId": "609a94df-e3f0-4eae-9982-03b11fbe3965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks"
      ],
      "metadata": {
        "id": "6A79QBdYwg4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "def predict(params, inputs):\n",
        "  for W, b in params:\n",
        "    outputs = jnp.dot(inputs, W) + b\n",
        "    inputs = jax.nn.relu(outputs)\n",
        "  return outputs\n",
        "\n",
        "def loss(params, batch):\n",
        "  inputs, targets = batch\n",
        "  predictions = predict(params, inputs)\n",
        "  return jnp.mean(jnp.sum((predictions - targets) ** 2, axis=-1))"
      ],
      "metadata": {
        "id": "ZRlH1rY9v9pm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_layer(key, n_in, n_out):\n",
        "  k1, k2 = jax.random.split(key)\n",
        "  W = jax.random.normal(k1, (n_in, n_out)) / jnp.sqrt(n_in)\n",
        "  b = jax.random.normal(k2, (n_out,))\n",
        "  return W, b\n",
        "\n",
        "def init(key, layer_sizes, batch_size):\n",
        "  key, *keys = jax.random.split(key, len(layer_sizes))\n",
        "  params = list(map(init_layer, keys, layer_sizes[:-1], layer_sizes[1:]))\n",
        "\n",
        "  key, *keys = jax.random.split(key, 3)\n",
        "  inputs = jax.random.normal(keys[0], (batch_size, layer_sizes[0]))\n",
        "  targets = jax.random.normal(keys[1], (batch_size, layer_sizes[-1]))\n",
        "\n",
        "  return params, (inputs, targets)"
      ],
      "metadata": {
        "id": "XRI-Fw7QxABN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_sizes = [784, 128, 128, 128, 128, 128, 8]\n",
        "batch_size = 32\n",
        "\n",
        "params, batch = init(jax.random.PRNGKey(0), layer_sizes, batch_size)"
      ],
      "metadata": {
        "id": "xvoVMEYExnMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While in automatic partitioning, we don't need to edit the model functions to use different parallelization strategies, with `shard_map` we often do."
      ],
      "metadata": {
        "id": "9S52iXN3x4iO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8-way batch data parallelism\n",
        "\n",
        " - shard the batch of inputs and targets over multiple devices\n",
        " - replicate the parameters over those devices\n",
        "\n",
        "To evaluate the total loss, the devices need only communicate with a scalar-sized all-reduce-sum at the end."
      ],
      "metadata": {
        "id": "yMnJqHcVyKzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "from jax.sharding import NamedSharding, Mesh, PartitionSpec as P\n",
        "from jax.experimental.shard_map import shard_map\n",
        "from jax.experimental import mesh_utils\n",
        "\n",
        "devices = mesh_utils.create_device_mesh((8,))"
      ],
      "metadata": {
        "id": "D2KfrzIFxu6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replicate initial params on all devices, shard data batch over devices\n",
        "mesh = Mesh(devices, ('batch',))\n",
        "batch = jax.device_put(batch, NamedSharding(mesh, P('batch')))\n",
        "params = jax.device_put(params, NamedSharding(mesh, P()))"
      ],
      "metadata": {
        "id": "2IDSIr1Gyp06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapt the loss fcn to sum the losses across devices\n",
        "def loss_dp(params, batch):\n",
        "  @partial(shard_map, mesh=mesh, in_specs=P('batch', None), out_specs=P())\n",
        "  def loss_spmd(local_batch):\n",
        "    inputs, targets = local_batch\n",
        "    predictions = predict(params, inputs) # local prediction\n",
        "    local_loss = jnp.mean(jnp.sum((predictions - targets) ** 2, axis=-1))\n",
        "    return jax.lax.pmean(local_loss, 'batch')\n",
        "  return loss_spmd(batch)"
      ],
      "metadata": {
        "id": "i9V_UL1-y9PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(jax.jit(loss)(params, batch))\n",
        "print(jax.jit(loss_dp)(params, batch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5N57pW2zoR5",
        "outputId": "2bff4f67-12de-4520-997c-e2cccc93bca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22.795706\n",
            "22.795706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def allclose(a, b):\n",
        "  return jax.tree_util.tree_all(jax.tree_map(partial(jnp.allclose, atol=1e-2, rtol=1e-2), a, b))"
      ],
      "metadata": {
        "id": "dzgMP6mOzuRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Q-2A5uu_0sE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "emX0P0Bz0r4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(allclose(jax.jit(jax.grad(loss))(params, batch),\n",
        "               jax.jit(jax.grad(loss_dp))(params, batch)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MunmmbP60a_2",
        "outputId": "b4627ad7-b201-4abc-ee94-017ca8923eb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8-way fully sharded data parallelism (FSDP)\n",
        "\n",
        "- Additionally shard the parameters over the devices, and all-gather each one when the full value is needed for the `jnp.dot` or bias addition.\n",
        "\n",
        "- So now we need collectives in two places: model prediction function `predict` needs to all-gather the parameters before they're used & as in the DP case the loss function needs to sum the local losses to compute the total loss\n",
        "\n",
        "- one more ingredient: we don't want to store the fully gathered parameters from the forward pass for use on backward pass. Instead, we want to gather them again on the backward pass. We can express that by using `jax.mat` with a custom policy though XLA typically does that rematerialization automatically.."
      ],
      "metadata": {
        "id": "t6fOAGEhFqqa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SE9P1cx6HQea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_layer(key, n_in, n_out):\n",
        "  k1, k2 = jax.random.split(key)\n",
        "  W = jax.random.normal(k1, (n_in, n_out)) / jnp.sqrt(n_in)\n",
        "  b = jax.random.normal(k2, (n_out,))\n",
        "  return W, b\n",
        "\n",
        "def init(key, layer_sizes, batch_size):\n",
        "  key, *keys = jax.random.split(key, len(layer_sizes))\n",
        "  params = list(map(init_layer, keys, layer_sizes[:-1], layer_sizes[1:]))\n",
        "\n",
        "  key, *keys = jax.random.split(key, 3)\n",
        "  inputs = jax.random.normal(keys[0], (batch_size, layer_sizes[0]))\n",
        "  targets = jax.random.normal(keys[1], (batch_size, layer_sizes[-1]))\n",
        "\n",
        "  return params, (inputs, targets)"
      ],
      "metadata": {
        "id": "KnAKZeTIG-XE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_sizes = [784, 128, 128, 128, 128, 128, 8]\n",
        "batch_size = 32\n",
        "\n",
        "params, batch = init(jax.random.PRNGKey(0), layer_sizes, batch_size)"
      ],
      "metadata": {
        "id": "8mF621ZgGyYL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "from jax.sharding import NamedSharding, Mesh, PartitionSpec as P\n",
        "from jax.experimental.shard_map import shard_map\n",
        "from jax.experimental import mesh_utils\n",
        "\n",
        "devices = mesh_utils.create_device_mesh((8,))"
      ],
      "metadata": {
        "id": "2VUfjhivHWWa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shard data batch and *and params* over devices\n",
        "mesh = Mesh(devices, ('batch',))\n",
        "batch = jax.device_put(batch, NamedSharding(mesh, P('batch')))\n",
        "params = jax.device_put(params, NamedSharding(mesh, P('batch')))"
      ],
      "metadata": {
        "id": "JVhfwTsA0izE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adapt the prediction function to gather weights just before their use,\n",
        "# and to re-gather them on the backward pass (rather than saving them)\n",
        "# *_ to express no interest in any further arguments\n",
        "@partial(jax.remat, policy=lambda op, *_, **__: str(op) != 'all_gather')\n",
        "def predict_fsdp(params_frag, inputs):\n",
        "  for W_frag, b_frag in params_frag:\n",
        "    W = jax.lax.all_gather(W_frag, 'batch', tiled=True)\n",
        "    b = jax.lax.all_gather(b_frag, 'batch', tiled=True)\n",
        "    outputs = jnp.dot(inputs, W) + b\n",
        "    inputs = jax.nn.relu(outputs)\n",
        "  return outputs\n",
        "\n",
        "def loss_fsdp(params, batch):\n",
        "  @partial(shard_map, mesh=mesh, in_specs=P('batch'), out_specs=P())\n",
        "  def loss_spmd(local_params, local_batch):\n",
        "    inputs, targets = local_batch\n",
        "    predictions = predict_fsdp(local_params, inputs)\n",
        "    local_loss = jnp.mean(jnp.sum((predictions - targets) ** 2, axis=-1))\n",
        "    return jax.lax.pmean(local_loss, 'batch')\n",
        "  return loss_spmd(params, batch)"
      ],
      "metadata": {
        "id": "OFc-swTwHPCi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(jax.jit(loss)(params, batch))"
      ],
      "metadata": {
        "id": "yKqvbMDtKiY8",
        "outputId": "5ca7195a-0d83-4d7e-b4a5-4c61e879bd36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22.795706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(jax.jit(loss_fsdp)(params, batch))"
      ],
      "metadata": {
        "id": "mXSNJsxIKsMr",
        "outputId": "42cfd605-b66c-45ac-ac15-0633359f402e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22.795706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def allclose(a, b):\n",
        "  return jax.tree_util.tree_all(jax.tree_map(partial(jnp.allclose, atol=1e-2, rtol=1e-2), a, b))"
      ],
      "metadata": {
        "id": "ojvhFgH2K7yL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(allclose(jax.jit(jax.grad(loss))(params, batch),\n",
        "               jax.jit(jax.grad(loss_fsdp))(params, batch)))"
      ],
      "metadata": {
        "id": "LLavr9_MKvEC",
        "outputId": "f7c7849f-c670-4504-cf1b-4fb95df25057",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8-way tensor parallelism\n",
        "\n",
        "Parallelization idea is that we'll keep the data/activations sharded over its feature axis (rather than its batch axis), and we'll similarly shard weight matrices over their input-feature axis (and biases over their feature values). Then to perform the parallel matrix multiplications, we'll perform local matrix multiplications followed by a `psum_scatter` to sum the local results and efficiently scatter the result's shards."
      ],
      "metadata": {
        "id": "dLwFLxrxLBhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mesh = Mesh(devices, ('feats',))\n",
        "\n",
        "batch = jax.device_put(batch, NamedSharding(mesh, P(None, 'feats')))\n",
        "params = jax.device_put(params, NamedSharding(mesh, P('feats')))\n",
        "\n",
        "def predict_tp(params, inputs):\n",
        "  for W, b in params:\n",
        "    outputs = gemm_tp(inputs, W, b)\n",
        "    inputs = jax.nn.relu(outputs)\n",
        "  return outputs\n",
        "\n",
        "@partial(shard_map, mesh=mesh, in_specs=(P(None, 'feats'), P('feats', None), P('feats')), out_specs=P(None, 'feats'))\n",
        "def gemm_tp(inputs, W, b):\n",
        "  block_result = jnp.dot(inputs, W)\n",
        "  return jax.lax.psum_scatter(block_result, 'feats', scatter_dimension=1, tiled=True) + b\n",
        "\n",
        "def loss_tp(params, batch):\n",
        "  inputs, targets = batch\n",
        "  predictions = predict_tp(params, inputs)\n",
        "  return jnp.mean(jnp.sum((predictions - targets) ** 2, axis=-1)) # NOTE psum!\n"
      ],
      "metadata": {
        "id": "7y57pSOvK1eT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(jax.jit(loss)(params, batch))"
      ],
      "metadata": {
        "id": "eDm1zjuwMxbr",
        "outputId": "1a98ed7a-7a9d-434f-92e4-b2f6860a3645",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22.795746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(jax.jit(loss_tp)(params, batch))"
      ],
      "metadata": {
        "id": "WiVTTl49M0oK",
        "outputId": "219ba9ff-7275-4fbb-cdfd-a7f4d084ebf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22.795746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(allclose(jax.jit(jax.grad(loss))(params, batch),\n",
        "               jax.jit(jax.grad(loss_tp))(params, batch)))"
      ],
      "metadata": {
        "id": "ITDzOyDQM4Uj",
        "outputId": "689b337b-d373-41b3-8421-258215b4299e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "20-ifg3KNpSs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}