{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONLjJsw6ju1aKpm0bnaIu1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/present42/PyTorchPractice/blob/main/Following_Flax_Tutorial_Managing_Parameters_and_State.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Managing Parameters and State"
      ],
      "metadata": {
        "id": "TT9OoB5L3DZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* manage the variables from initialization to updates\n",
        "* split and re-assemble parameters and state\n",
        "* use `vmap` with batch-dependent state"
      ],
      "metadata": {
        "id": "TPVVYm7R3JP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flax import linen as nn"
      ],
      "metadata": {
        "id": "RjaOeQk_5E9e"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "import jax"
      ],
      "metadata": {
        "id": "uyxUCgI95I1E"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kOouGLM4201p"
      },
      "outputs": [],
      "source": [
        "class BiasAdderWithRunningMean(nn.Module):\n",
        "  momentum: float = 0.9\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    is_initialized = self.has_variable('batch_stats', 'mean')\n",
        "    mean = self.variable('batch_stats', 'mean', jnp.zeros, x.shape[1:])\n",
        "    bias = self.param('bias', lambda rng, shape: jnp.zeros(shape), x.shape[1:])\n",
        "    if is_initialized:\n",
        "      mean.value = (self.momentum * mean.value +\n",
        "                    (1.0 - self.momentum) * jnp.mean(x, axis=0, keepdims=True))\n",
        "    return mean.value + bias"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tricky part with init here is that we need to split the state variables and the parameters we're going to optimize for."
      ],
      "metadata": {
        "id": "EgiMGSET5A8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we define `update_step` as follows (with a dummy loss that should be replaced with yours):"
      ],
      "metadata": {
        "id": "IQ2BXieG5QM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.random as random\n",
        "import flax\n",
        "import optax"
      ],
      "metadata": {
        "id": "5iJGzcpB6QpF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = random.key(0)\n",
        "key, i_key, m_key = random.split(key, 3)\n",
        "model = BiasAdderWithRunningMean()\n",
        "\n",
        "dummy_input = random.normal(i_key, (10,)) # Dummy input data\n",
        "variables = model.init(m_key, dummy_input)\n",
        "print(variables)\n",
        "\n",
        "# flax.core.pop creates a new FrozenDict where one entry is removed\n",
        "# returns a pair with the new FrozenDict and the removed value\n",
        "state, params = flax.core.pop(variables, 'params')\n",
        "print(variables)\n",
        "del variables # delete variables to avoid wasting resources\n",
        "tx = optax.sgd(learning_rate=0.02)\n",
        "opt_state = tx.init(params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5THP9vF6H8V",
        "outputId": "35cc4e02-3688-4afd-f7f8-ff8ac1371fc1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_stats': {'mean': Array(0., dtype=float32)}, 'params': {'bias': Array(0., dtype=float32)}}\n",
            "{'batch_stats': {'mean': Array(0., dtype=float32)}, 'params': {'bias': Array(0., dtype=float32)}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def update_step(apply_fn, x, opt_state, params, state):\n",
        "  def loss(params):\n",
        "    y, updated_state = apply_fn({'params': params, **state},\n",
        "                                x, mutable=list(state.keys()))\n",
        "    l = ((x - y) ** 2).sum() # replace with your loss here\n",
        "    return l, updated_state\n",
        "\n",
        "  (l, updated_state), grads = jax.value_and_grad(\n",
        "      loss, has_aux=True)(params)\n",
        "  updates, opt_state = tx.update(grads, opt_state) # update gradient\n",
        "  params = optax.apply_updates(params, updates) # recompute params\n",
        "  return opt_state, params, updated_state"
      ],
      "metadata": {
        "id": "18EzNped4_we"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for _ in range(num_epochs):\n",
        "  opt_state, params, state = update_step(\n",
        "      model.apply, dummy_input, opt_state, params, state\n",
        "  )"
      ],
      "metadata": {
        "id": "2zklBhl77lk0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `vmap` across the batch dimension"
      ],
      "metadata": {
        "id": "5XLDnwxZ6Ex4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  hidden_size: int\n",
        "  out_size: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, train=False):\n",
        "    norm = partial(\n",
        "        nn.BatchNorm,\n",
        "        use_running_average=not train,\n",
        "        momentum=0.9,\n",
        "        epsilon=1e-5,\n",
        "        axis_name=\"batch\" # Name batch dim\n",
        "    )\n",
        "\n",
        "    x = nn.Dense(self.hidden_size)(x)\n",
        "    x = norm()(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.Dense(self.hidden_size)(x)\n",
        "    x = norm()(x)\n",
        "    x = nn.relu(x)\n",
        "    y = nn.Dense(self.out_size)(x)\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "HZ4z9Gta9VD0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Secondly, we need to specify the same name when calling `vmap` in our training code:"
      ],
      "metadata": {
        "id": "mi4U6QKr6En0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_step(apply_fn, x_batch, y_batch, opt_state, params, state):\n",
        "  def batch_loss(params):\n",
        "    def loss_fn(x, y):\n",
        "      pred, updated_state = apply_fn(\n",
        "          {'params': params, **state},\n",
        "          x, mutable=list(state.keys())\n",
        "      )\n",
        "      return (pred - y) ** 2, updated_state\n",
        "\n",
        "    loss, updated_state = jax.vmap(\n",
        "        loss_fn, out_axes=(0, None), # Do not vmap `updated_state`\n",
        "        axis_name='batch' # Name batch dim\n",
        "    )(x_batch, y_batch)\n",
        "    return jnp.mean(loss), updated_state\n",
        "\n",
        "  (loss, updated_state), grads = jax.value_and_grad(\n",
        "      batch_loss, has_aux=True\n",
        "  )(params)\n",
        "\n",
        "  updates, opt_state = tx.update(grads, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return opt_state, params, updated_state, loss\n",
        "\n"
      ],
      "metadata": {
        "id": "AmomWXE7-PCl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note that we also need to specify that the model state does not have a batch dimension"
      ],
      "metadata": {
        "id": "3c1MN3mt_N-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(hidden_size=10, out_size=1)\n",
        "variables = model.init(m_key, dummy_input)\n",
        "# split state and params\n",
        "state, params = flax.core.pop(variables, 'params')\n",
        "del variables # delete variables to avoid wasting resources\n",
        "tx = optax.sgd(learning_rate=0.02)\n",
        "opt_state = tx.init(params)\n",
        "\n",
        "for _ in range(num_epochs):\n",
        "  opt_state, params, state, loss = update_step(\n",
        "      model.apply, X, Y, opt_state, params, state\n",
        "  )"
      ],
      "metadata": {
        "id": "Sy7xEhpc_K41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YmWabok2_Xnk"
      }
    }
  ]
}