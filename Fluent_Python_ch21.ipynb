{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1drnpcU2MJGpuFW8ZwtPL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 21. Asynchronous Programming"
      ],
      "metadata": {
        "id": "1gyUhusueJ3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Three major topics that are related:\n",
        " - Python's `async def`, `await`, `async with`, and `async for`\n",
        " - Objects supporting those constructs: native coroutines and asynchronous variants of context managers, iterables, generators, and comprehensions\n",
        " - `asyncio` and other async libraries\n"
      ],
      "metadata": {
        "id": "Q-95Qxg_eUj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Few Definitions\n",
        "\n",
        "Native coroutine\n",
        " - a coroutine fcn defined with `async def`. You can delegate from a native coroutine to another native coroutine using the `await` keyword, similar to `yield from`. The `await` keyword cannot be used outside of a native coroutine.\n",
        "\n",
        "Classic coroutine\n",
        " - A generator function that consumes datat sent to it via `my_coro.send(data)` calls, and reads that data by using `yield` in an expression. Classic coroutines can delegate to other classic coroutines using `yield from`.\n",
        "\n",
        "Generator-based coroutine\n",
        " - A generator fcn decorated with `@types.coroutine`. That decorator makes the generator compatible with the new `await` keyword.\n",
        "\n",
        "Asynchronous generator\n",
        " - A generator fcn defined with `async def` and using `yield` in its body. It returns an async generator object that provides `_anext_`, a coroutine method to retrieve the next itme."
      ],
      "metadata": {
        "id": "MFDqqeZYfRb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10:09 - 10:25"
      ],
      "metadata": {
        "id": "VQxVt7Mlz1dG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3fujZoJeDnx"
      },
      "outputs": [],
      "source": [
        "# blogdom.py\n",
        "\n",
        "#!/usr/bin/env pythono3\n",
        "import asyncio\n",
        "import socket\n",
        "from keyword import kwlist\n",
        "\n",
        "MAX_KEYWORD_LEN = 4\n",
        "\n",
        "async def probe(domain: str) -> tuple[str, bool]:\n",
        "  # get a reference to the asyncio event loop\n",
        "  # so we can use it next\n",
        "  loop = asyncio.get_running_loop()\n",
        "  try:\n",
        "    # loop.getaddrinfo coroutine method\n",
        "    # returns a five-part tuple of paramters\n",
        "    await loop.getaddrinfo(domain, None)\n",
        "  except socket.gaierror:\n",
        "    return (domain, False)\n",
        "  return (domain, True)\n",
        "\n",
        "# main must be a coroutine so that we can use await in it\n",
        "async def main() -> None:\n",
        "  names = (kw for kw in kwlist if len(kw) <= MAX_KEYWORD_LEN)\n",
        "  domains = (f'{name}.dev'.lower() for name in names)\n",
        "  # build a list of coroutine objs by invoking\n",
        "  # the probe coroutine with each domain argument\n",
        "  coros = [probe(domain) for domain in domains]\n",
        "  # asyncio.as_completed is a generator that yields\n",
        "  # coroutines that return the results of the coroutine\n",
        "  # passed to it **in the order they are completed**\n",
        "  for coro in asyncio.as_completed(coros):\n",
        "    # await expression will not block\n",
        "    # but this is required for us to get the res from coro\n",
        "    domain, found = await coro\n",
        "    mark = '+' if found else ' '\n",
        "    print(f'{mark} {domain}')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # asyncio.run starts the event loop\n",
        "  # and returns only when the event loop exits\n",
        "  asyncio.run(main())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python blogdom.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13Jj2Z3l1Sb4",
        "outputId": "0011e9d5-9f46-44b9-f901-2d2fa96a4ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+ not.dev\n",
            "+ as.dev\n",
            "+ from.dev\n",
            "  is.dev\n",
            "  with.dev\n",
            "  for.dev\n",
            "+ def.dev\n",
            "  or.dev\n",
            "  none.dev\n",
            "  pass.dev\n",
            "  if.dev\n",
            "+ in.dev\n",
            "  elif.dev\n",
            "+ and.dev\n",
            "+ try.dev\n",
            "+ true.dev\n",
            "  else.dev\n",
            "+ del.dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Awaitable\n",
        "\n",
        "The `for` keyword works with `iterables`. The `await` keyword works with `awaitable`.\n",
        "\n",
        "As the end user of `asyncio`, these are awaitables we'll see frequently:\n",
        " - A native coroutine object, which you get by calling a native coroutine function\n",
        " - An `asyncio.Task` which we usually get by passing a coroutine obj to `asyncio.create_task()`\n",
        "\n",
        "Lower-level awaitable:\n",
        " - An object with an `__await__` method that returns an iterator; for example, an `asyncio.Future` instance\n",
        "\n",
        " - Objects written in ohter languages using the Python/C API with a `tp_as_async.am_await` function returning an iterator"
      ],
      "metadata": {
        "id": "PFjHW34UuX4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading with asyncio and HTTPX"
      ],
      "metadata": {
        "id": "AxvKLd1WvrEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As of Python 3.10, `asyncio` only supports TCP and UDP directly, and there are no async HTTP client or server packages in the standard library. So, we use HTTPX."
      ],
      "metadata": {
        "id": "UQxaHX2YvyDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# flags_asyncio.py\n",
        "\n",
        "import asyncio\n",
        "\n",
        "from httpx import AsyncClient\n",
        "\n",
        "from flags import BASE_URL, save_flag, main\n",
        "\n",
        "# must be a native coroutine\n",
        "# so it can await on get_flag which does http request\n",
        "# Then, it displays the code of the downloaded flag,\n",
        "# and save the result\n",
        "async def download_one(client: AsyncClient, cc: str):\n",
        "  image = await get_flag(client, cc)\n",
        "  save_flag(image, f'{cc}.gif')\n",
        "  print(cc, end=' ', flush=True)\n",
        "  return cc\n",
        "\n",
        "async def get_flag(client: AsyncClient, cc: str) -> bytes:\n",
        "  url = f\"{BASE_URL}/{cc}/{cc}.gif\".lower()\n",
        "  # get method of httpx.AsyncClient returns a ClientResponse obj\n",
        "  # that is also an async context manager\n",
        "  resp = await client.get(url, timeout=6.1, follow_redirects=True)\n",
        "  return resp.read()\n",
        "\n",
        "# This fcn needs to be a plain fcn--not a coroutine\n",
        "def download_many(cc_list: list[str]) -> int:\n",
        "  # execute the event loop driving the supervisor(cc_list)\n",
        "  # coroutine object until it returns\n",
        "  return asyncio.run(supervisor(cc_list))\n",
        "\n",
        "async def supervisor(cc_list: list[str]) -> int:\n",
        "  # Async HTTP client operations in httpx are methods of AsyncClient\n",
        "  # which is also an async context manager\n",
        "  # a context manager with async setup and teardown methods\n",
        "  async with AsyncClient() as client:\n",
        "    # build a list of coroutine objects by calling the download_one\n",
        "    # coroutine once for each flag to be retrieved\n",
        "    to_do = [download_one(client, cc) for cc in sorted(cc_list)]\n",
        "    # wait for the asyncio.gather coroutine,\n",
        "    # which accepts one or more awaitable arguments\n",
        "    # and waits for all of them to complete\n",
        "    res = await asyncio.gather(*to_do)\n",
        "\n",
        "  return len(res)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main(download_many)"
      ],
      "metadata": {
        "id": "TZ7TRvC4uirh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Secret of Native Coroutines: Humble Generators"
      ],
      "metadata": {
        "id": "3BV3WvkGypYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A key difference between the classic coroutine and native coroutine is that there are no visible `.send()` calls or `yield` expression in the latter. Our code sits between the asyncio library and the async libraries we are using (e.g. HTTPX).\n",
        "\n",
        "Under the hood, the `asyncio` event loop makes the `.send` calls that drive your coroutines, and your coroutines `await` on other coroutines, including library coroutines. (`await` borrows most of its implementation from `yield from`, which also makes `.send` calls to drive coroutines)\n",
        "\n"
      ],
      "metadata": {
        "id": "7uY1-K6IyzGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All-or-Nothing Problem"
      ],
      "metadata": {
        "id": "FfVPXSnNz_cr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Asynchronous Context Managers"
      ],
      "metadata": {
        "id": "v83yVjOR0PuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In an async driver like asyncpg, the setup and wrap-up need to be coroutines so that other operations can happen concurrently. However, the implementation of the classic `with` statement doesn't support coroutines doing the work of `__enter__` or `__exit__`.\n",
        "\n",
        "In the `asyncpg.Transaction` class, the `__aenter__` coroutine method does `await` `self.start()` and the `__aexit__` coroutine awaits on private `__rollback` or `__commit` coroutine methods, depending on whether an exception occurred or not."
      ],
      "metadata": {
        "id": "ybkdUU-b6831"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Back to *flags_asyncio.py*, the `AsyncClient` class of `httpx` is an async context manager, so it can use awaitables in its `__aenter__` and `__aexit__` special coroutine methods."
      ],
      "metadata": {
        "id": "UUoQaHhR70og"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enhancing the asyncio Downloader\n"
      ],
      "metadata": {
        "id": "CEpPSOyX8RTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# slow_server.py\n",
        "\n",
        "\"\"\" Slow HTTP server class.\n",
        "\n",
        "This module implements a ThreadingHTTPServer using a custom\n",
        "SimpleHTTPRequestHandler subclass that introduces delays to all\n",
        "GET responses, and optionally returns erros to a fraction of\n",
        "the requests if given the --error_rate command-line argument\n",
        "\"\"\"\n",
        "\n",
        "import contextlib\n",
        "import os\n",
        "import socket\n",
        "import time\n",
        "from functools import partial\n",
        "from http import server, HTTPStatus\n",
        "from http.server import ThreadingHTTPServer, SimpleHTTPRequestHandler\n",
        "from random import random, uniform\n",
        "\n",
        "MIN_DELAY = 0.5 # minimum delay for do_GET (secs)\n",
        "MAX_DELAY = 5.0 # maximum delay for do_GET (secs)\n",
        "\n",
        "class SlowHTTPRequestHandler(SimpleHTTPRequestHandler):\n",
        "  \"\"\"\n",
        "  The optional error_rate arg determines how often GET requests\n",
        "  receive a 418 status code\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, *args, error_rate=0.0, **kwargs):\n",
        "    self.error_rate = error_rate\n",
        "    super().__init__(*args, **kwargs)\n",
        "\n",
        "  def do_GET(self):\n",
        "    \"\"\"Serve a GET request.\"\"\"\n",
        "    delay = uniform(MIN_DELAY, MAX_DELAY)\n",
        "    cc = self.path[-6:-4].upper()\n",
        "    print(f'{cc} delay: {delay:0.2}s')\n",
        "    time.sleep(delay)\n",
        "    if random() < self.error_rate:\n",
        "      try:\n",
        "        self.send_error(HTTPStatus.IM_A_TEAPOT, \"I'm a Teapot\")\n",
        "      except BrokenPipeError as exc:\n",
        "        print(f\"{cc} *** BrokenPipeError: client closed\")\n",
        "    else:\n",
        "      f = self.send_head()\n",
        "      if f:\n",
        "        try:\n",
        "          self.copyfile(f, self.wfile)\n",
        "        except BrokenPipeError as exc:\n",
        "          print(f\"{cc} *** BrokenPipeError: client closed\")\n",
        "        finally:\n",
        "          f.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  import argparse\n",
        "\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--bind', '-b', metavar='ADDRESS',\n",
        "                      help='Specify alternate bind address '\n",
        "                           '[default: all interfaces]')\n",
        "  parser.add_argument('--directory', '-d', default=os.getcwd(),\n",
        "                      help='Specify alternative directory '\n",
        "                           '[default:current directory]')\n",
        "  parser.add_argument('--error-rate', '-e', metavar='PROBABILITY',\n",
        "                      default=0.0, type=float,\n",
        "                      help='Error rate; e.g. use .25 for 25% probability '\n",
        "                           '[default:0.0]')\n",
        "  parser.add_argument('port', metavar='store',\n",
        "                      default=8001, type=int,\n",
        "                      nargs='?',\n",
        "                      help='Specify alternate port [default: 8001]')\n",
        "  args = parser.parse_args()\n",
        "  handler_class = partial(SlowHTTPRequestHandler,\n",
        "                          directory=args.directory,\n",
        "                          error_rate=args.error_rate)\n",
        "\n",
        "  class DualStackServer(ThreadingHTTPServer):\n",
        "    def server_bind(self):\n",
        "      with contextlib.suppress(Exception):\n",
        "        self.socket.setpockopt(\n",
        "            socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 0\n",
        "        )\n",
        "        return super().server_bind()\n",
        "\n",
        "    server.test(\n",
        "        HandlerClass=handler_class,\n",
        "        ServerClass=DualStackServer,\n",
        "        port=args.port,\n",
        "        bind=args.bind,\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "bMXEIWZU1To6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using asyncio.as_completed and a Thread\n",
        "\n",
        "In the previous example, we passed several coroutines to `asyncio.gather` which returns a list with results of the coroutines in the order they were submitted.\n",
        "\n",
        "This means that `asyncio.gather` can only return when all the waitables are done."
      ],
      "metadata": {
        "id": "VUnilqw9EjnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# flags2_asyncio.py\n",
        "\n",
        "import asyncio\n",
        "from collections import Counter\n",
        "from http import HTTPStatus\n",
        "from pathlib import Path\n",
        "\n",
        "import httpx\n",
        "import tqdm\n",
        "\n",
        "from flags2_common import main, DownloadStatus, save_flag\n",
        "\n",
        "# low concurrency default to avoid errors from remote site\n",
        "# such as 503 - Service Temporarily Unavailable\n",
        "DEFAULT_CONCUR_REQ = 5\n",
        "MAX_CONCUR_REQ = 1000\n",
        "\n",
        "# get_flag is very similar to the sequential version\n",
        "# 1st difference: it requires the client parameter\n",
        "async def get_flag(client: httpx.AsyncClient,\n",
        "                   base_url: str,\n",
        "                   cc: str) -> bytes:\n",
        "  url = f'{base_url}/{cc}/{cc}.gif'.lower()\n",
        "  # 2nd difference: `.get` is an `AsyncClient` method\n",
        "  #                 and it's coroutine so we need to `await` it\n",
        "  resp = await client.get(url, timeout=3.1, follow_redirects=True)\n",
        "  resp.raise_for_status()\n",
        "  return resp.content\n",
        "\n",
        "async def download_one(client: httpx.AsyncClient,\n",
        "                       cc: str,\n",
        "                       base_url: str,\n",
        "                       semaphore: asyncio.Semaphore,\n",
        "                       verbose: bool) -> DownloadStatus:\n",
        "  try:\n",
        "    # use semaphore as an async context manager so that\n",
        "    # the program as a whole is not blocked\n",
        "    async with semaphore:\n",
        "      image = await get_flag(client, base_url, cc)\n",
        "  except:\n",
        "    res = exc.response\n",
        "    if res.status_code == HTTPStatus.NOT_FOUND:\n",
        "      status = DownloadStatus.NOT_FOUND\n",
        "      msg = f\"not found: {res.url}\"\n",
        "    else:\n",
        "      raise\n",
        "  else:\n",
        "    # Saving the image is an I/O operation\n",
        "    # To avoid blocking the event loop, run save_flag in a thread\n",
        "    await asyncio.to_thread(save_flag, image, f\"{cc}.gif\")\n",
        "    status = DownloadStatus.OK\n",
        "    msg = 'OK'\n",
        "  if verbose and msg:\n",
        "    print(cc, msg)\n",
        "  return status\n",
        "\n",
        "# Supervisor takes the same arguments as the `download_many` fnc\n",
        "# but it cannot be invoked directly from main because it's a\n",
        "# coroutine and not a plain function like download_many\n",
        "async def supervisor(cc_list: list[str],\n",
        "                     base_url: str,\n",
        "                     verbose: bool,\n",
        "                     concur_req: int) -> Counter[DonwloadStatus]:\n",
        "  counter: Counter[DownloadStatus] = Counter()\n",
        "  semaphore = asyncio.Semaphore(concur_req)\n",
        "  async with httpx.AsyncClient() as client:\n",
        "    # create a list of coroutine objects,\n",
        "    # one per call to the download_one coroutine\n",
        "\n",
        "    to_do = [download_one(client, cc, base_url, semaphore, verbose)\n",
        "            for cc in sorted(cc_list)]\n",
        "    to_do_iter = asyncio.as_completed(to_do)\n",
        "    if not verbose:\n",
        "      to_do_iter = tqdm.tqdm(to_do_iter, total=len(cc_list))\n",
        "    error: httpx.HTTPError | None = None\n",
        "    for coro in to_do_iter:\n",
        "      try:\n",
        "        status = await coro\n",
        "      except httpx.HTTPStatusError as exc:\n",
        "        error_msg = 'HTTP error {resp.status_code} - {resp.reason_phrase}'\n",
        "        error_msg = error_msg.format(resp=exc.response)\n",
        "        error = exc\n",
        "      except httpx.RequestError as exc:\n",
        "        error_msg = f'{exc} {type(exc)}'.strip()\n",
        "        error = exc\n",
        "      except KeyboardInterrupt:\n",
        "        break\n",
        "\n",
        "      if error:\n",
        "        status = DownloadStatus.ERROR\n",
        "        if verbose:\n",
        "          url = str(error.request.url)\n",
        "          cc = Path(url).stem.upper()\n",
        "          print(f'{cc} error: {error_msg}')\n",
        "      counter[status] += 1\n",
        "\n",
        "  return counter\n",
        "\n",
        "def download_many(cc_list: list[str],\n",
        "                  base_url: str,\n",
        "                  verbose: bool,\n",
        "                  concur_req: int) -> Counter[DownloadStatus]:\n",
        "  coro = supervisor(cc_list, base_url, verbose, concur_req)\n",
        "  # instantiates the supervisor coroutine object\n",
        "  # and passes it to the event loop with asyncio.run\n",
        "  counts = asyncio.run(coro)\n",
        "\n",
        "  return counts\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main(download_many, DEFAULT_CONCUR_REQ, MAX_CONCUR_REQ)"
      ],
      "metadata": {
        "id": "bFu6ChyWEfJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All network I/O is done with coroutine in `asyncio`, but not file I/O. However, file I/O is also \"blocking\"--in the sense that r/w files takes thousands of times longer than r/w to RAM.\n",
        "\n",
        "Since Python 3.9, the `asyncio.to_thread` coroutine makes it easy to delegate file I/O to a thread pool provided by `asyncio`."
      ],
      "metadata": {
        "id": "XeZRRPh6HjZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making Multiple Requests for Each Download\n",
        "\n",
        "Suppose we want to save each country flag with the name of the country and the coutry code. Now we need to make 2 HTTP requests per flag: one to get the flag image itself, the other to get the *metadata.json* file in the same directory as the image."
      ],
      "metadata": {
        "id": "QabavlPgGsPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `await` keyword allows you to drive the async requests one after the other, sharing the local scope of the driving coroutine."
      ],
      "metadata": {
        "id": "EHz0Y_npHQtn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few changes in third variation of the `asyncio` flag downloading script:\n",
        "\n",
        "- `get_country`\n",
        " * This new coroutine fetches the `metadata.json` file for the country code, and gets the name of the country from it.\n",
        "\n",
        "- `download_one`\n",
        " * This coroutine now uses `await` to delegate to `get_flag` and the new `get_country` coroutine, using the result of the latter to build the name of the file to save"
      ],
      "metadata": {
        "id": "oaUVLqhIHfz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# flags3_asyncio.py\n",
        "\n",
        "import asyncio\n",
        "from collections import Counter\n",
        "from http import HTTPStatus\n",
        "from pathlib import Path\n",
        "\n",
        "import httpx\n",
        "import tqdm\n",
        "\n",
        "from flags2_common import main, DownloadStatus, save_flag\n",
        "\n",
        "# low concurrency default to avoid errors from remote site\n",
        "# such as 503 - Service Temporarily Unavailable\n",
        "DEFAULT_CONCUR_REQ = 5\n",
        "MAX_CONCUR_REQ = 1000\n",
        "\n",
        "\n",
        "async def get_country(client: httpx.AsyncClient,\n",
        "                      base_url: str,\n",
        "                      cc: str) -> str:\n",
        "    url = f'{base_url}/{cc}/metadata.json'\n",
        "    resp = await client.get(url, timeout=3.1, follow_redirects=True)\n",
        "    resp.raise_for_status()\n",
        "    metadata = resp.json()\n",
        "    return metadata['country']\n",
        "\n",
        "# get_flag is very similar to the sequential version\n",
        "# 1st difference: it requires the client parameter\n",
        "async def get_flag(client: httpx.AsyncClient,\n",
        "                   base_url: str,\n",
        "                   cc: str) -> bytes:\n",
        "  url = f'{base_url}/{cc}/{cc}.gif'.lower()\n",
        "  # 2nd difference: `.get` is an `AsyncClient` method\n",
        "  #                 and it's coroutine so we need to `await` it\n",
        "  resp = await client.get(url, timeout=3.1, follow_redirects=True)\n",
        "  resp.raise_for_status()\n",
        "  return resp.content\n",
        "\n",
        "async def download_one(client: httpx.AsyncClient,\n",
        "                       cc: str,\n",
        "                       base_url: str,\n",
        "                       semaphore: asyncio.Semaphore,\n",
        "                       verbose: bool) -> DownloadStatus:\n",
        "  try:\n",
        "    async with semaphore:\n",
        "      image = await get_flag(client, base_url, cc)\n",
        "    async with semaphore:\n",
        "      country = await get_country(client, base_url, cc)\n",
        "  except httpx.HTTPStatusError as exc:\n",
        "    res = exc.response\n",
        "    if res.stats_code == HTTPStatus.NOT_FOUND:\n",
        "      status = DownloadStatus.NOT_FOUND\n",
        "      msg = f'not found: {res.url}'\n",
        "    else:\n",
        "      raise\n",
        "  else:\n",
        "    filename = country.replace(' ', '_')\n",
        "    await asyncio.to_thread(save_flag, image, f'{filename}.gif')\n",
        "    status = DownloadStatus.OK\n",
        "    msg = 'OK'\n",
        "\n",
        "  if verbose and msg:\n",
        "    print(cc, msg)\n",
        "  return status\n",
        "\n",
        "# Supervisor takes the same arguments as the `download_many` fnc\n",
        "# but it cannot be invoked directly from main because it's a\n",
        "# coroutine and not a plain function like download_many\n",
        "async def supervisor(cc_list: list[str],\n",
        "                     base_url: str,\n",
        "                     verbose: bool,\n",
        "                     concur_req: int) -> Counter[DownloadStatus]:\n",
        "  counter: Counter[DownloadStatus] = Counter()\n",
        "  semaphore = asyncio.Semaphore(concur_req)\n",
        "  async with httpx.AsyncClient() as client:\n",
        "    # create a list of coroutine objects,\n",
        "    # one per call to the download_one coroutine\n",
        "\n",
        "    to_do = [download_one(client, cc, base_url, semaphore, verbose)\n",
        "            for cc in sorted(cc_list)]\n",
        "    to_do_iter = asyncio.as_completed(to_do)\n",
        "    if not verbose:\n",
        "      to_do_iter = tqdm.tqdm(to_do_iter, total=len(cc_list))\n",
        "    error: httpx.HTTPError | None = None\n",
        "    for coro in to_do_iter:\n",
        "      try:\n",
        "        status = await coro\n",
        "      except httpx.HTTPStatusError as exc:\n",
        "        error_msg = 'HTTP error {resp.status_code} - {resp.reason_phrase}'\n",
        "        error_msg = error_msg.format(resp=exc.response)\n",
        "        error = exc\n",
        "      except httpx.RequestError as exc:\n",
        "        error_msg = f'{exc} {type(exc)}'.strip()\n",
        "        error = exc\n",
        "      except KeyboardInterrupt:\n",
        "        break\n",
        "\n",
        "      if error:\n",
        "        status = DownloadStatus.ERROR\n",
        "        if verbose:\n",
        "          url = str(error.request.url)\n",
        "          cc = Path(url).stem.upper()\n",
        "          print(f'{cc} error: {error_msg}')\n",
        "      counter[status] += 1\n",
        "\n",
        "  return counter\n",
        "\n",
        "def download_many(cc_list: list[str],\n",
        "                  base_url: str,\n",
        "                  verbose: bool,\n",
        "                  concur_req: int) -> Counter[DownloadStatus]:\n",
        "  coro = supervisor(cc_list, base_url, verbose, concur_req)\n",
        "  # instantiates the supervisor coroutine object\n",
        "  # and passes it to the event loop with asyncio.run\n",
        "  counts = asyncio.run(coro)\n",
        "\n",
        "  return counts\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main(download_many, DEFAULT_CONCUR_REQ, MAX_CONCUR_REQ)"
      ],
      "metadata": {
        "id": "h0OaFJEMH0k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing asyncio Servers"
      ],
      "metadata": {
        "id": "Z-N47h7nJDR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# charindex.py\n",
        "\n",
        "import sys\n",
        "import unicodedata\n",
        "from collections import defaultdict\n",
        "from collections.abc import Iterator\n",
        "\n",
        "STOP_CODE: int = sys.maxunicode + 1\n",
        "Char = str\n",
        "Index = defaultdict[str, set[Char]]\n",
        "\n",
        "def tokenize(text: str) -> Iterator[str]:\n",
        "  \"\"\"return iterator of uppercased words\"\"\"\n",
        "  for word in text.upper().replace('-', ' ').split():\n",
        "    yield word\n",
        "\n",
        "class InvertedIndex:\n",
        "  entries: Index\n",
        "\n",
        "  def __init__(self, start: int = 32, stop: int = STOP_CODE):\n",
        "    entries: Index = defaultdict(set)\n",
        "    for char in (chr(i) for i in range(start, stop)):\n",
        "      name = unicodedata.name(char, '')\n",
        "      if name:\n",
        "        for word in tokenize(name):\n",
        "          entries[word].add(char)\n",
        "\n",
        "    self.entries = entries\n",
        "\n",
        "  def search(self, query: str) -> set[str]:\n",
        "    if words := list(tokenize(query)):\n",
        "      found = self.entries[words[0]]\n",
        "      return found.intersection(*(self.entries[w] for w in words[1:]))\n",
        "    else:\n",
        "      return set()\n",
        "\n",
        "def format_results(chars: set[Char]) -> Iterator[str]:\n",
        "  for char in sorted(chars):\n",
        "    name = unicodedata.name(char)\n",
        "    code = ord(char)\n",
        "    yield f'U+{code:04x}\\t{char}\\t{name}'\n",
        "\n",
        "def main(words: list[str]) -> None:\n",
        "  if not words:\n",
        "    print('Please give one or more words to search')\n",
        "    sys.exit(2)\n",
        "  index = InvertedIndex()\n",
        "  chars = index.search(' '.join(words))\n",
        "  for line in format_results(chars):\n",
        "    print(line)\n",
        "  print('-' * 66, f'{len(chars)} found')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main(sys.argv[1:])"
      ],
      "metadata": {
        "id": "SQGR-4GlDCrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# web_mojifinder.py\n",
        "\n",
        "from pathlib import Path\n",
        "from unicodedata import name\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from fastapi.responses import HTMLResponse\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from charindex import InvertedIndex\n",
        "\n",
        "STATIC_PATH = Path(__file__).parent.absolute()\n",
        "\n",
        "app = FastAPI(\n",
        "    title='Mojifinder Web',\n",
        "    description='Search for Unicode characters by name.'\n",
        ")\n",
        "\n",
        "class CharName(BaseModel): # Pydantic schema for a JSON response\n",
        "  char: str\n",
        "  name: str\n",
        "\n",
        "def init(app):\n",
        "  app.state.index = InvertedIndex()\n",
        "  app.state.form = (STATIC_PATH / 'form.html').read_text()\n",
        "\n",
        "# run init when this module is loaded by the ASGI server\n",
        "init(app)\n",
        "\n",
        "# FastAPI assumes that any parameters that appear in the fcn\n",
        "# or coroutine signature that are not in the route path will be\n",
        "# passed in the HTTP query string\n",
        "@app.get('/search', response_model=list[CharName])\n",
        "async def search(q: str):\n",
        "  chars = sorted(app.state.index.search(q))\n",
        "  # return an iterable of dicts compatible with the response_model\n",
        "  # schema allows FastAPI to build the JSON response\n",
        "  return ({'char': c, 'name': name(c)} for c in chars)\n",
        "\n",
        "@app.get('/', response_class=HTMLResponse, include_in_schema=False)\n",
        "def form():\n",
        "  return app.state.form\n",
        "\n",
        "# no main function\n",
        "# it is loaded and driven by the ASGI server (e.g., uvicorn)"
      ],
      "metadata": {
        "id": "dAzo383NIx3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b1uJi1EBJfRw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}